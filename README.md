# Understanding Multi-Task Activities from Single-Task Videos  
*CVPR 2025 Highlight Â· Yuhan Shen Â· Ehsan Elhamifar*

[**ğŸ“„ Read the Paper (PDF)**](https://openaccess.thecvf.com/content/CVPR2025/papers/Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos_CVPR_2025_paper.pdf) â€ƒ
[**ğŸ–¼ï¸ View the Poster (PDF)**](https://yuhan-shen.github.io/files/cvpr25_mttas_poster.pdf)



## Introduction

We introduce **Multiâ€‘Task Temporal Action Segmentation (MTâ€‘TAS)**, a novel framework designed to tackle the complex scenario of interleaved actions in multiâ€‘task videos using only singleâ€‘task training data. By leveraging innovative modules, including **Multiâ€‘task Sequence Blending**, **Segment Boundary Learning**, **Dynamic Isolation of Video Elements**, **Foreground-Background Feature Composition**, and **Foregroundâ€‘Aware Action Refinement**, the approach synthesizes multiâ€‘task videos and dynamically isolates foreground/background elements to improve multi-task temporal segmentation. To validate its effectiveness, we present the **MEKA** dataset, a 12â€‘hour egocentric multiâ€‘task kitchen activity collection, demonstrating that MTâ€‘TAS significantly narrows the performance gap between singleâ€‘task training and multiâ€‘task evaluation, achieving stateâ€‘ofâ€‘theâ€‘art results in complex, realâ€‘world environments


> **We are preparing an open-source release of our codes and the MEKA dataset. Please stay tuned!**
